{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f2a2af-74b0-470d-93fe-953f8e953754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.signal as signal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Reshape, Input\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f682ab-91ba-4415-afe4-c69203a0f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3ca414-fc81-43e6-a29a-137d6729e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Add functionality to handle joining multiple csvs together from different file locations into usable df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d27d7d5-9e1b-4208-8e03-847a0859508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_7056\\4056852923.py:6: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(real_path, header=None)\n"
     ]
    }
   ],
   "source": [
    "#  Example df with correct columns\n",
    "\n",
    "ex_path = \"C:/Users/Eric/Downloads/emg_dataframe.csv\"\n",
    "real_path = \"C:/Users/Eric/Downloads/1738796078.0934057.csv\"\n",
    "\n",
    "df = pd.read_csv(real_path, header=None)\n",
    "df.columns = df.iloc[0]\n",
    "df = df.drop(0, axis=0).reset_index(drop=True)\n",
    "\n",
    "#rand_data = np.random.rand(1000, len(df.columns))\n",
    "#df_rand = pd.DataFrame(rand_data, columns=df.columns)\n",
    "\n",
    "#df = df.dropna(axis=1, how='all')\n",
    "#df_rand = df_rand.dropna(axis=1, how='all')\n",
    "\n",
    "# Concatenate the dataframes\n",
    "#df = pd.concat([df, df_rand], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e94992cb-d633-461f-8c5d-d8b3c8dea8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for reference rn\n",
    "data = {\n",
    "    'row': [0],\n",
    "    'time': [1, 10, 19, 28, 37],\n",
    "    'emg': [2, 11, 20, 29, 38],\n",
    "    'imp': [3, 12, 21, 30, 39],\n",
    "    'accx': [4, 13, 22, 31, 40],\n",
    "    'accy': [5, 14, 23, 32, 41],\n",
    "    'accz': [6, 15, 24, 33, 42],\n",
    "    'gyrx': [7, 16, 25, 34, 43],\n",
    "    'gyry': [8, 17, 26, 35, 44],\n",
    "    'gyrz': [9, 18, 27, 36, 45],\n",
    "}\n",
    "start_row = 110\n",
    "end_row_emg = 396800\n",
    "end_row_other = 46700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70af4b9c-2a54-49db-9dbc-93f6f71a6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = [col for col in df.columns if isinstance(col, str) and ('IMP' in col.upper() or 'TIME' in col.upper()) and col != 'time 0']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01f49c36-f922-4e7c-8606-ddd3f9f2097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[200:390000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f462e866-361f-4d06-8eec-397351ed6505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time 0', 'sensor 0 EMG 1 (1259.259 Hz)', 'sensor 0 ACC X (148.148 Hz)',\n",
       "       'sensor 0 ACC Y (148.148 Hz)', 'sensor 0 ACC Z (148.148 Hz)',\n",
       "       'sensor 0 GYRO X (148.148 Hz)', 'sensor 0 GYRO Y (148.148 Hz)',\n",
       "       'sensor 0 GYRO Z (148.148 Hz)', 'sensor 1 EMG 1 (1259.259 Hz)',\n",
       "       'sensor 1 ACC X (148.148 Hz)', 'sensor 1 ACC Y (148.148 Hz)',\n",
       "       'sensor 1 ACC Z (148.148 Hz)', 'sensor 1 GYRO X (148.148 Hz)',\n",
       "       'sensor 1 GYRO Y (148.148 Hz)', 'sensor 1 GYRO Z (148.148 Hz)',\n",
       "       'sensor 2 EMG 1 (1259.259 Hz)', 'sensor 2 ACC X (148.148 Hz)',\n",
       "       'sensor 2 ACC Y (148.148 Hz)', 'sensor 2 ACC Z (148.148 Hz)',\n",
       "       'sensor 2 GYRO X (148.148 Hz)', 'sensor 2 GYRO Y (148.148 Hz)',\n",
       "       'sensor 2 GYRO Z (148.148 Hz)', 'sensor 3 EMG 1 (1259.259 Hz)',\n",
       "       'sensor 3 ACC X (148.148 Hz)', 'sensor 3 ACC Y (148.148 Hz)',\n",
       "       'sensor 3 ACC Z (148.148 Hz)', 'sensor 3 GYRO X (148.148 Hz)',\n",
       "       'sensor 3 GYRO Y (148.148 Hz)', 'sensor 3 GYRO Z (148.148 Hz)',\n",
       "       'sensor 4 EMG 1 (1259.259 Hz)', 'sensor 4 ACC X (148.148 Hz)',\n",
       "       'sensor 4 ACC Y (148.148 Hz)', 'sensor 4 ACC Z (148.148 Hz)',\n",
       "       'sensor 4 GYRO X (148.148 Hz)', 'sensor 4 GYRO Y (148.148 Hz)',\n",
       "       'sensor 4 GYRO Z (148.148 Hz)', 'sensor 5 EMG 1 (1259.259 Hz)',\n",
       "       'sensor 5 ACC X (148.148 Hz)', 'sensor 5 ACC Y (148.148 Hz)',\n",
       "       'sensor 5 ACC Z (148.148 Hz)', 'sensor 5 GYRO X (148.148 Hz)',\n",
       "       'sensor 5 GYRO Y (148.148 Hz)', 'sensor 5 GYRO Z (148.148 Hz)'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c6e7315-c788-4782-8a20-97ea5b02597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emg_columns = ['sensor 0 EMG 1 (1259.259 Hz)',\n",
    "               'sensor 1 EMG 1 (1259.259 Hz)',\n",
    "               'sensor 2 EMG 1 (1259.259 Hz)',\n",
    "               'sensor 3 EMG 1 (1259.259 Hz)',\n",
    "               'sensor 4 EMG 1 (1259.259 Hz)',\n",
    "               'sensor 5 EMG 1 (1259.259 Hz)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0456e83c-c23c-4994-8e36-90506335f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df, target_fs):\n",
    "    \"\"\"\n",
    "    Resample data to the target sampling frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame containing the time and signal data (e.g., 'time 0', EMG, GYRO, ACC columns).\n",
    "    - target_fs: The target sampling frequency (Hz).\n",
    "    \n",
    "    Returns:\n",
    "    - df_resampled: A new DataFrame with all columns resampled to the target frequency.\n",
    "    \"\"\"\n",
    "    # Ensure 'time 0' is the time column and handle NaN values\n",
    "    time_column = df['time 0'].dropna()  # Remove NaN values\n",
    "    if len(time_column) < 2:\n",
    "        raise ValueError(\"Time column 'time 0' has fewer than 2 valid entries after removing NaN.\")\n",
    "    \n",
    "    # Calculate total duration and original sampling frequency\n",
    "    total_duration = time_column.iloc[-1] - time_column.iloc[0]  # Total time span\n",
    "    if np.isnan(total_duration):\n",
    "        raise ValueError(\"Total duration is NaN. Check 'time 0' for invalid values.\")\n",
    "    \n",
    "    original_length = len(time_column)  # Number of valid samples in time column\n",
    "    original_fs = original_length / total_duration  # Average original sampling frequency (Hz)\n",
    "    \n",
    "    # Calculate target length based on target_fs\n",
    "    target_length = int(total_duration * target_fs)  # Number of samples at target frequency\n",
    "    if target_length <= 0:\n",
    "        raise ValueError(f\"Target length {target_length} is invalid. Check total_duration and target_fs.\")\n",
    "    \n",
    "    new_time = np.linspace(time_column.iloc[0], time_column.iloc[-1], target_length)  # New time array\n",
    "    \n",
    "    # Initialize resampled DataFrame with the new time column\n",
    "    df_resampled = pd.DataFrame({'time 0': new_time})\n",
    "    \n",
    "    # Identify signal columns\n",
    "    emg_columns = [col for col in df.columns if 'EMG' in col]\n",
    "    gyro_accel_columns = [col for col in df.columns if 'GYRO' in col or 'ACC' in col]\n",
    "    \n",
    "    # Resample all signal columns to the target length\n",
    "    for col in emg_columns + gyro_accel_columns:\n",
    "        original_signal = df[col].dropna()  # Remove NaN values from signal\n",
    "        if len(original_signal) < 2:\n",
    "            print(f\"Warning: Column '{col}' has fewer than 2 valid entries. Filling with zeros.\")\n",
    "            df_resampled[col] = np.zeros(target_length)\n",
    "            continue\n",
    "        \n",
    "        # Interpolate to match time column length if needed\n",
    "        if len(original_signal) != original_length:\n",
    "            original_time = np.linspace(time_column.iloc[0], time_column.iloc[-1], len(original_signal))\n",
    "            interpolated_signal = np.interp(time_column, original_time, original_signal)\n",
    "            resampled_signal = signal.resample(interpolated_signal, target_length)\n",
    "        else:\n",
    "            resampled_signal = signal.resample(original_signal, target_length)\n",
    "        df_resampled[col] = resampled_signal\n",
    "    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a54bb9a-937e-45c7-9761-6187ecbd6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implement butterworth filters of different orders, cutoff frequencies, sampling frequencies, and filter_band type to columns of a dataframe.\n",
    "\n",
    "def butterworth_filter(data, order, cutoff, fs, filter_band='low'):\n",
    "    nyquist = 0.5 * fs\n",
    "    normalized_cutoff = np.array(cutoff) / nyquist\n",
    "    \n",
    "    #  Create Butterworth filter coefficients\n",
    "    if filter_band == 'low':\n",
    "        b, a = signal.butter(order, normalized_cutoff[0], btype='low')\n",
    "    elif filter_band == 'high':\n",
    "        b, a = signal.butter(order, normalized_cutoff[0], btype='high')\n",
    "    elif filter_band == 'bandpass':\n",
    "        b, a = signal.butter(order, normalized_cutoff, btype='bandpass')\n",
    "    elif filter_band == 'bandstop':\n",
    "        b, a = signal.butter(order, normalized_cutoff, btype='bandstop')\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported filter: {filter_band}\")\n",
    "    \n",
    "    #  Apply filter\n",
    "    filtered_data = signal.filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "def apply_filter(df, columns, order=4, cutoff=[1.0], fs=1000, filter_band='low'):\n",
    "    '''\n",
    "    Apply a Butterworth filter of specified order and cutoff to the given columns of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataframe containing signal data.\n",
    "    - columns: List of columns to apply the filter to (e.g., EMG columns).\n",
    "    - order: The order of the filter.\n",
    "    - cutoff: The cutoff frequency or frequencies.\n",
    "    - fs: Sampling frequency of the signal.\n",
    "    - filter_band: Type of filter ('low', 'high', 'bandpass', 'bandstop').\n",
    "    \n",
    "    Returns:\n",
    "    - df: The DataFrame with filtered columns.\n",
    "    '''\n",
    "    for col in columns:\n",
    "        df[col] = butterworth_filter(df[col].values, order, cutoff, fs, filter_band)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21fb97fa-37ca-4127-a2e1-e24d8e2a3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_notch(df, columns, notch_freq=60, fs=1000, Q=30):\n",
    "    '''\n",
    "    Apply a notch filter to remove a specific frequency (e.g., powerline noise at 60Hz)\n",
    "    to the specified columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The dataframe containing signal data.\n",
    "    - columns: List of columns to apply the notch filter to.\n",
    "    - notch_freq: Frequency to remove with the notch filter (e.g., 60Hz for powerline noise).\n",
    "    - fs: Sampling frequency of the signal.\n",
    "    - Q: Quality factor for the notch filter.\n",
    "    \n",
    "    Returns:\n",
    "    - df: The DataFrame with filtered columns.\n",
    "    '''\n",
    "    nyquist = 0.5 * fs\n",
    "    normalized_frequency = notch_freq / nyquist\n",
    "\n",
    "    b, a = signal.iirnotch(normalized_frequency, Q)\n",
    "\n",
    "    for col in columns:\n",
    "        df[col] = signal.filtfilt(b, a, df[col].values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aed76a5c-0660-4599-aa39-863a60c2c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    time_column = df.columns[0]\n",
    "    \n",
    "    sensor_columns = df.columns[1:]\n",
    "\n",
    "    left_keywords = ['sensor 0', 'sensor 1', 'sensor 2']\n",
    "\n",
    "    left_sensors = [col for col in sensor_columns if any(keyword in col for keyword in left_keywords)]\n",
    "    right_sensors = [col for col in sensor_columns if col not in left_sensors]\n",
    "\n",
    "    left_emg = [col for col in left_sensors if 'EMG' in col]\n",
    "    right_emg = [col for col in right_sensors if 'EMG' in col]\n",
    "\n",
    "    left_inputs = [col for col in left_sensors if col not in left_emg]\n",
    "    right_inputs = [col for col in right_sensors if col not in right_emg]\n",
    "\n",
    "    return df[left_inputs], df[left_emg], df[right_inputs], df[right_emg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36c1541e-5cea-4faa-b5b6-036f69192967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(features, targets, window_size, target_size, step_size):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(0, len(features) - window_size - target_size + 1, step_size):\n",
    "        X.append(features.iloc[i:i + window_size].values)\n",
    "        y.append(targets.iloc[i + window_size:i + window_size + target_size].values)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "778abf88-37f0-42dc-93c7-10cbd3611d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Linear interpolation for nulls (idk if we have nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f3865de-8d72-4e9c-8449-fa9ea39b48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lets go\n"
     ]
    }
   ],
   "source": [
    "#  Drop table if any column missing\n",
    "if len(df.columns) < 43:\n",
    "    print(\"yikes! having missing data is quite embarassing\")\n",
    "else:\n",
    "    print(\"lets go\")\n",
    "\n",
    "#  Scale data with minimax\n",
    "columns_non_time = df.columns[1:]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df[columns_non_time] = scaler_minmax.fit_transform(df[columns_non_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46956e21-932d-4b2a-a34f-037fa6921383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Resample data\n",
    "\n",
    "df = resample_data(df, target_fs=100)\n",
    "\n",
    "#  Apply butterworth filter\n",
    "df = apply_filter(df, emg_columns)\n",
    "\n",
    "#  Apply notch filter\n",
    "df = apply_notch(df, emg_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da00f6d-c8ed-42a5-8c87-7907d153d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_sensors, left_emg, right_sensors, right_emg = split_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b2cf9d9-2c19-48f3-904d-42ee12b8c7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31400, 18)\n",
      "(31400, 3)\n",
      "(31400, 18)\n",
      "(31400, 3)\n"
     ]
    }
   ],
   "source": [
    "print(left_sensors.shape)\n",
    "print(left_emg.shape)\n",
    "print(right_sensors.shape)\n",
    "print(right_emg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9411d31-025c-4e74-8bd0-78e422fdaf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_left shape: (31386, 10, 18)\n",
      "y_right shape: (31386, 5, 3)\n",
      "X_right shape: (31386, 10, 18)\n",
      "y_left shape: (31386, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "step_size = 1\n",
    "target_size = 5\n",
    "\n",
    "X_left, y_left = create_sliding_windows(left_sensors, right_emg, window_size, target_size, step_size)\n",
    "X_right, y_right = create_sliding_windows(right_sensors, left_emg, window_size, target_size, step_size)\n",
    "\n",
    "print(\"X_left shape:\", X_left.shape)\n",
    "print(\"y_right shape:\", y_right.shape)\n",
    "print(\"X_right shape:\", X_right.shape)\n",
    "print(\"y_left shape:\", y_left.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afe9f2b6-466c-4282-adbe-0ed11a7f53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1569/1570 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 1: val_loss improved from inf to 0.00005, saving model to model.h5\n",
      "1570/1570 [==============================] - 24s 11ms/step - loss: 0.0060 - val_loss: 4.9149e-05\n",
      "Epoch 2/20\n",
      "1566/1570 [============================>.] - ETA: 0s - loss: 7.4718e-04\n",
      "Epoch 2: val_loss improved from 0.00005 to 0.00003, saving model to model.h5\n",
      "1570/1570 [==============================] - 18s 11ms/step - loss: 7.4653e-04 - val_loss: 3.4598e-05\n",
      "Epoch 3/20\n",
      "1569/1570 [============================>.] - ETA: 0s - loss: 2.2563e-04\n",
      "Epoch 3: val_loss improved from 0.00003 to 0.00003, saving model to model.h5\n",
      "1570/1570 [==============================] - 17s 11ms/step - loss: 2.2560e-04 - val_loss: 2.6016e-05\n",
      "Epoch 4/20\n",
      "1569/1570 [============================>.] - ETA: 0s - loss: 8.4143e-05\n",
      "Epoch 4: val_loss improved from 0.00003 to 0.00001, saving model to model.h5\n",
      "1570/1570 [==============================] - 17s 11ms/step - loss: 8.4141e-05 - val_loss: 1.1740e-05\n",
      "Epoch 5/20\n",
      "1566/1570 [============================>.] - ETA: 0s - loss: 5.6388e-05\n",
      "Epoch 5: val_loss did not improve from 0.00001\n",
      "1570/1570 [==============================] - 17s 11ms/step - loss: 5.6388e-05 - val_loss: 1.6245e-05\n",
      "Epoch 6/20\n",
      "1567/1570 [============================>.] - ETA: 0s - loss: 4.6489e-05\n",
      "Epoch 6: val_loss did not improve from 0.00001\n",
      "1570/1570 [==============================] - 15s 10ms/step - loss: 4.6479e-05 - val_loss: 2.6096e-05\n",
      "Epoch 7/20\n",
      "1565/1570 [============================>.] - ETA: 0s - loss: 4.0335e-05\n",
      "Epoch 7: val_loss improved from 0.00001 to 0.00001, saving model to model.h5\n",
      "1570/1570 [==============================] - 16s 10ms/step - loss: 4.0312e-05 - val_loss: 7.2246e-06\n",
      "Epoch 8/20\n",
      "1457/1570 [==========================>...] - ETA: 0s - loss: 3.7000e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#  Train on combined data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined_with_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m          \u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_combined_with_indicator)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#  Predict right EMG from left sensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  Combine data to generalize\n",
    "X_combined = np.concatenate([X_left, X_right], axis=0)\n",
    "y_combined = np.concatenate([y_left, y_right], axis=0)\n",
    "\n",
    "#  Add side indicator (0 = left sensors, 1 = right sensors)\n",
    "side_indicator = np.concatenate([np.zeros(len(X_left)), np.ones(len(X_right))])\n",
    "side_indicator = side_indicator.reshape(-1, 1, 1).repeat(window_size, axis=1)\n",
    "\n",
    "#  Combine sensor data with side indicator\n",
    "X_combined_with_indicator = np.concatenate([X_combined, side_indicator], axis=2)\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model.h5\", save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "\n",
    "#  Generalized LSTM\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(window_size, 19)))\n",
    "model.add(LSTM(units=64, return_sequences=False))\n",
    "model.add(Dropout(0.2))  #  Stop overfitting\n",
    "model.add(Dense(units=target_size * 3))  # Output 3 EMG values\n",
    "model.add(Reshape((target_size, 3)))     # Shape: (samples, target_size, 3)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#  Train on combined data\n",
    "model.fit(X_combined_with_indicator, \n",
    "          y_combined, epochs=20, \n",
    "          batch_size=32, \n",
    "          validation_split=0.2, \n",
    "          callbacks=[checkpoint])\n",
    "\n",
    "predictions = model.predict(X_combined_with_indicator)\n",
    "\n",
    "#  Predict right EMG from left sensors\n",
    "X_new_left, _ = create_sliding_windows(left_sensors, right_emg, window_size, target_size, step_size)\n",
    "side_indicator_new = np.zeros((X_new_left.shape[0], window_size, 1))  # 0 for left\n",
    "X_new_with_indicator = np.concatenate([X_new_left, side_indicator_new], axis=2)\n",
    "pred_right_emg = model.predict(X_new_with_indicator)\n",
    "#  Predict left EMG from right sensors\n",
    "X_new_right, _ = create_sliding_windows(right_sensors, left_emg, window_size, target_size, step_size)\n",
    "side_indicator_new = np.ones((X_new_right.shape[0], window_size, 1))  # 1 for right\n",
    "X_new_with_indicator = np.concatenate([X_new_right, side_indicator_new], axis=2)\n",
    "pred_left_emg = model.predict(X_new_with_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29e7bad7-bb5c-4f28-bdf1-e6808d80d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gait_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58e448f4-67f0-4770-ad73-fc7c6b12f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Data augmentation ?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d30a617-39c3-4213-a6f9-cc349e1aedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Evaluate performance of different filters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
